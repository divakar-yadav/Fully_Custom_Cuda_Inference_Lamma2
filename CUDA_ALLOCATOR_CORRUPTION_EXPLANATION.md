# CUDA Allocator Corruption: Root Cause Analysis

## üö® Problem Statement

After capturing approximately **150-200 CUDA graphs**, the system encounters:
```
[Capture] ‚ö†Ô∏è  WARNING: CUDA allocator corruption at seq_len=151 (retry 1/5)
Error type: CUDA internal assert/captures_underway - attempting recovery...
```

This eventually leads to:
```
[Capture] ‚ùå SKIP seq_len=151 after 5 retries (CUDA allocator corruption persists)
```

## üîç Root Cause: PyTorch CUDA Graph Memory Management

### 1. **Per-Graph Memory Allocation**

Each CUDA graph capture allocates **persistent device memory**:

```python
# In _capture_graph_on_cpp_stream():
wrapper = StaticModelForward(self.model, seq_len, self.device, self.capture_stream)
# Creates:
#   - static_input_ids: (1, seq_len) tensor on GPU
#   - static_position_ids: (1, seq_len) tensor on GPU  
#   - static_logits: (1, seq_len, vocab_size) tensor on GPU (allocated during forward)

graph = torch.cuda.CUDAGraph()
with torch.cuda.graph(graph, stream=self.capture_stream):
    wrapper.forward()  # Captures ALL operations and their memory allocations
```

**Key Point**: Each graph capture creates:
- **Input tensors**: ~seq_len * 4 bytes (int32)
- **Output logits**: seq_len * vocab_size * 2 bytes (float16)
- **Intermediate activations**: Captured in the graph's memory pool
- **Graph metadata**: PyTorch's internal tracking structures

### 2. **CUDA Allocator's Internal State Tracking**

PyTorch's CUDA allocator maintains:
- **Free memory blocks**: Linked lists of available GPU memory
- **Allocated blocks**: Tracking structures for each allocation
- **Graph-specific pools**: Memory pools dedicated to each CUDA graph
- **Capture state**: Tracking which graphs are "underway"

After **~150 graph captures**, these internal structures become:
1. **Fragmented**: Many small allocations scattered across GPU memory
2. **Inconsistent**: Tracking structures become out of sync with actual memory state
3. **Corrupted**: Internal linked lists or hash tables have broken pointers

### 3. **Why It Fails Around Sequence Length 150**

The failure happens because:

```
Memory per graph ‚âà seq_len √ó (vocab_size √ó 2 bytes + intermediate activations)
For seq_len=150: ~150 √ó (32,000 √ó 2 + activations) ‚âà 10-20 MB per graph
150 graphs √ó 20 MB ‚âà 3 GB of tracked allocations

Beyond this point:
- CUDA allocator's internal tracking structures overflow
- Memory fragmentation prevents efficient allocation
- Internal asserts trigger: "captures_underway.empty() INTERNAL ASSERT FAILED"
```

### 4. **The "captures_underway" Error**

This specific error occurs when:
- PyTorch's CUDA allocator has **multiple graphs in capture state** (due to corrupted tracking)
- The allocator tries to **allocate memory** but finds inconsistent state
- Internal assertion: `captures_underway.empty()` fails because tracking is corrupted

**Why recovery fails**:
- `torch.cuda.empty_cache()` can't clear graph-allocated memory (it's persistent)
- `torch.cuda.synchronize()` doesn't fix corrupted tracking structures
- The corruption is in **PyTorch's allocator state**, not CUDA driver state

## üìä Memory Growth Pattern

```
Graph # | Memory (approx) | Status
--------|----------------|----------
1-50    | ~500 MB        | ‚úÖ Works perfectly
51-100  | ~1 GB          | ‚úÖ Works well
101-150 | ~1.5 GB        | ‚ö†Ô∏è  Starts showing slowdown
151-200 | ~2 GB          | ‚ùå Allocator corruption begins
200+    | ~2.5+ GB       | ‚ùå Consistent failures
```

## üîß Why Current Fixes Don't Work

### Attempted Solutions:

1. **`torch.cuda.empty_cache()`**: 
   - ‚ùå Doesn't clear graph-allocated memory (graphs hold references)

2. **`torch.cuda.synchronize()`**: 
   - ‚ùå Doesn't fix corrupted tracking structures

3. **Retry with delays**:
   - ‚ùå Corruption persists; retries don't clear allocator state

4. **Skipping failed graphs**:
   - ‚ö†Ô∏è Workaround, but limits max sequence length

## üí° Potential Solutions (Not Implemented)

### Option 1: Graph Pool Rotation
- Capture graphs in "batches" of 150
- After batch is exhausted, delete all graphs and re-capture
- **Tradeoff**: Requires re-capturing overhead

### Option 2: Memory-Aware Graph Management  
- Track memory usage per graph
- Delete old graphs when approaching limit
- **Tradeoff**: Need LRU eviction (user explicitly said no)

### Option 3: Use TensorRT-LLM Style Approach
- Pre-compile graphs for fixed sequence lengths
- Don't capture dynamically
- **Tradeoff**: Less flexible

### Option 4: Reduce Memory per Graph
- Use smaller vocab_size for intermediate tensors
- Use quantization (INT8)
- **Tradeoff**: Accuracy/performance impact

### Option 5: Fix PyTorch Allocator (Requires PyTorch Changes)
- Modify PyTorch's CUDA allocator to handle many graphs
- This is a **PyTorch bug/limitation**, not our code issue

## üéØ Current Workaround

The system:
1. Pre-captures 150 graphs upfront
2. Attempts to capture more in background
3. Skips graphs that fail due to allocator corruption
4. Fails gracefully if generation needs a skipped graph

**This limits generation to ~150 tokens reliably**, with occasional success up to ~200 tokens before hitting corruption.

## üìù Conclusion

The CUDA allocator corruption is a **fundamental limitation of PyTorch's CUDA graph implementation** when capturing many graphs. It's not a bug in our code, but rather:

1. **PyTorch's allocator** doesn't efficiently handle 150+ persistent graph allocations
2. **Memory fragmentation** accumulates with each graph capture
3. **Internal tracking structures** become corrupted beyond ~150 graphs

**This is a known limitation** and would require either:
- PyTorch team to fix the allocator
- Using a different graph management strategy
- Accepting the ~150 graph limit for this approach

